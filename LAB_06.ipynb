{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992db60-996f-4039-9253-175237b3f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def function_modules(dataset_path):\n",
    "    \"\"\"\n",
    "    Function that performs the following:\n",
    "    1. Load dataset from a given path.\n",
    "    2. Perform summation using inputs and weights.\n",
    "    3. Apply different activation functions.\n",
    "    4. Compare the output with the expected value and calculate error.\n",
    "\n",
    "    :param dataset_path: Path to the dataset file on your laptop.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load dataset (assuming an Excel file with columns for embeddings and 'output')\n",
    "    try:\n",
    "        df = pd.read_excel(dataset_path)  # Use read_excel for Excel files\n",
    "        print(\"Dataset loaded successfully.\")\n",
    "        print(\"Column names:\", df.columns)  # Print column names for debugging\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {dataset_path} was not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Selecting two embedding columns and the output column\n",
    "    input_columns = ['embed_0', 'embed_1']  # Adjust these as necessary\n",
    "    output_column = 'output'\n",
    "\n",
    "    # Ensure that the required columns are present\n",
    "    if not all(col in df.columns for col in input_columns + [output_column]):\n",
    "        print(f\"Error: Dataset must contain the following columns: {input_columns + [output_column]}\")\n",
    "        return\n",
    "\n",
    "    # Initialize weights and bias (these can be adjusted or learned in practice)\n",
    "    weights = np.array([0.5, 0.5])  # Example weights for the two inputs\n",
    "    bias = -0.5  # Example bias term\n",
    "\n",
    "    # Lists to store the outputs and errors\n",
    "    step_outputs = []\n",
    "    sigmoid_outputs = []\n",
    "    tanh_outputs = []\n",
    "    relu_outputs = []\n",
    "    leaky_relu_outputs = []\n",
    "    errors = []\n",
    "\n",
    "    # Activation functions\n",
    "    def step_activation(weighted_sum):\n",
    "        return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "    def sigmoid_activation(weighted_sum):\n",
    "        return 1 / (1 + np.exp(-weighted_sum))\n",
    "\n",
    "    def tanh_activation(weighted_sum):\n",
    "        return np.tanh(weighted_sum)\n",
    "\n",
    "    def relu_activation(weighted_sum):\n",
    "        return max(0, weighted_sum)\n",
    "\n",
    "    def leaky_relu_activation(weighted_sum, alpha=0.01):\n",
    "        return weighted_sum if weighted_sum >= 0 else alpha * weighted_sum\n",
    "\n",
    "    def error_comparator(actual_output, expected_output):\n",
    "        return np.mean((actual_output - expected_output) ** 2)\n",
    "\n",
    "    # Step 2: Iterate through the dataset and perform calculations\n",
    "    for index, row in df.iterrows():\n",
    "        inputs = np.array([row[input_columns[0]], row[input_columns[1]]])\n",
    "        expected_output = row[output_column]\n",
    "\n",
    "        # Summation Unit\n",
    "        weighted_sum = np.dot(inputs, weights) + bias\n",
    "\n",
    "        # Apply each activation function\n",
    "        output_step = step_activation(weighted_sum)\n",
    "        output_sigmoid = sigmoid_activation(weighted_sum)\n",
    "        output_tanh = tanh_activation(weighted_sum)\n",
    "        output_relu = relu_activation(weighted_sum)\n",
    "        output_leaky_relu = leaky_relu_activation(weighted_sum)\n",
    "\n",
    "        # Error calculation for the Step function (can be extended for others)\n",
    "        error = error_comparator(output_step, expected_output)\n",
    "\n",
    "        # Store results\n",
    "        step_outputs.append(output_step)\n",
    "        sigmoid_outputs.append(output_sigmoid)\n",
    "        tanh_outputs.append(output_tanh)\n",
    "        relu_outputs.append(output_relu)\n",
    "        leaky_relu_outputs.append(output_leaky_relu)\n",
    "        errors.append(error)\n",
    "\n",
    "        print(f\"Row {index}: Inputs = {inputs}, Expected = {expected_output}, \"\n",
    "              f\"Step = {output_step}, Error = {error}\")\n",
    "\n",
    "    # Step 3: Return the results as a DataFrame for further analysis\n",
    "    result_df = pd.DataFrame({\n",
    "        'embed_0': df[input_columns[0]],\n",
    "        'embed_1': df[input_columns[1]],\n",
    "        'Expected': df[output_column],\n",
    "        'Step_Output': step_outputs,\n",
    "        'Sigmoid_Output': sigmoid_outputs,\n",
    "        'TanH_Output': tanh_outputs,\n",
    "        'ReLU_Output': relu_outputs,\n",
    "        'Leaky_ReLU_Output': leaky_relu_outputs,\n",
    "        'Error': errors\n",
    "    })\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(result_df)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def perceptron_and_gate():\n",
    "    \"\"\"\n",
    "    A perceptron to learn AND gate logic using a step activation function. \n",
    "    It will update weights based on the learning rate and plot the sum-square-error after each epoch.\n",
    "    \"\"\"\n",
    "    # Initialize weights and bias\n",
    "    W0 = 10    # Bias\n",
    "    W1 = 0.2   # Weight for first input\n",
    "    W2 = -0.75  # Weight for second input\n",
    "    alpha = 0.05  # Learning rate\n",
    "\n",
    "    # Training data for AND gate (input1, input2, expected_output)\n",
    "    training_data = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    # Step activation function\n",
    "    def step_activation(weighted_sum):\n",
    "        return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "    # Lists to store the sum of square errors and epochs for plotting\n",
    "    errors = []\n",
    "    epochs = []\n",
    "    epoch_count = 0\n",
    "\n",
    "    # Training loop\n",
    "    while True:\n",
    "        total_error = 0\n",
    "        for row in training_data:\n",
    "            input1 = row[0]\n",
    "            input2 = row[1]\n",
    "            expected_output = row[2]\n",
    "\n",
    "            # Weighted sum: W0*1 (bias) + W1*input1 + W2*input2\n",
    "            weighted_sum = W0 + W1 * input1 + W2 * input2\n",
    "\n",
    "            # Step activation function output\n",
    "            output = step_activation(weighted_sum)\n",
    "\n",
    "            # Error calculation (difference between expected output and actual output)\n",
    "            error = expected_output - output\n",
    "\n",
    "            # Weight updates (delta rule)\n",
    "            W0 += alpha * error * 1        # Update bias (as 1 is the input to bias)\n",
    "            W1 += alpha * error * input1   # Update weight for input1\n",
    "            W2 += alpha * error * input2   # Update weight for input2\n",
    "\n",
    "            # Sum square error calculation\n",
    "            total_error += error ** 2\n",
    "\n",
    "        # Append the total error for this epoch\n",
    "        errors.append(total_error)\n",
    "        epochs.append(epoch_count)\n",
    "\n",
    "        # Check if the learning has converged (error < 0.002) or if max epochs (1000) are reached\n",
    "        if total_error <= 0.002 or epoch_count >= 1000:\n",
    "            break\n",
    "\n",
    "        # Increment epoch counter\n",
    "        epoch_count += 1\n",
    "\n",
    "    # Print final weights and bias\n",
    "    print(f\"Final Weights and Bias after {epoch_count} epochs:\")\n",
    "    print(f\"W0 (Bias): {W0}, W1: {W1}, W2: {W2}\")\n",
    "\n",
    "    # Plotting the sum-square error versus epochs\n",
    "    plt.plot(epochs, errors, label=\"Sum-Square Error\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Sum-Square Error\")\n",
    "    plt.title(\"Perceptron Training - Epochs vs Error\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perceptron_with_activation(activation_type):\n",
    "    \"\"\"\n",
    "    A perceptron to learn AND gate logic using different activation functions.\n",
    "    It will update weights based on the learning rate and plot the sum-square-error after each epoch.\n",
    "    \n",
    "    :param activation_type: The type of activation function ('bi-polar', 'sigmoid', 'relu').\n",
    "    \"\"\"\n",
    "    # Initialize weights and bias\n",
    "    W0 = 10    # Bias\n",
    "    W1 = 0.2   # Weight for first input\n",
    "    W2 = -0.75  # Weight for second input\n",
    "    alpha = 0.05  # Learning rate\n",
    "\n",
    "    # Training data for AND gate (input1, input2, expected_output)\n",
    "    training_data = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    # Activation functions\n",
    "    def bi_polar_step(weighted_sum):\n",
    "        if weighted_sum > 0:\n",
    "            return 1\n",
    "        elif weighted_sum == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def sigmoid(weighted_sum):\n",
    "        return 1 / (1 + np.exp(-weighted_sum))\n",
    "\n",
    "    def relu(weighted_sum):\n",
    "        return max(0, weighted_sum)\n",
    "\n",
    "    # Function to return appropriate activation function based on input\n",
    "    if activation_type == 'bi-polar':\n",
    "        activation_function = bi_polar_step\n",
    "    elif activation_type == 'sigmoid':\n",
    "        activation_function = sigmoid\n",
    "    elif activation_type == 'relu':\n",
    "        activation_function = relu\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation function type\")\n",
    "\n",
    "    # Lists to store the sum of square errors and epochs for plotting\n",
    "    errors = []\n",
    "    epochs = []\n",
    "    epoch_count = 0\n",
    "\n",
    "    # Training loop\n",
    "    while True:\n",
    "        total_error = 0\n",
    "        for row in training_data:\n",
    "            input1 = row[0]\n",
    "            input2 = row[1]\n",
    "            expected_output = row[2]\n",
    "\n",
    "            # Weighted sum: W0*1 (bias) + W1*input1 + W2*input2\n",
    "            weighted_sum = W0 + W1 * input1 + W2 * input2\n",
    "\n",
    "            # Activation function output\n",
    "            output = activation_function(weighted_sum)\n",
    "\n",
    "            # Error calculation (difference between expected output and actual output)\n",
    "            error = expected_output - output\n",
    "\n",
    "            # Weight updates (delta rule)\n",
    "            W0 += alpha * error * 1        # Update bias (as 1 is the input to bias)\n",
    "            W1 += alpha * error * input1   # Update weight for input1\n",
    "            W2 += alpha * error * input2   # Update weight for input2\n",
    "\n",
    "            # Sum square error calculation\n",
    "            total_error += error ** 2\n",
    "\n",
    "        # Append the total error for this epoch\n",
    "        errors.append(total_error)\n",
    "        epochs.append(epoch_count)\n",
    "\n",
    "        # Check if the learning has converged (error < 0.002) or if max epochs (1000) are reached\n",
    "        if total_error <= 0.002 or epoch_count >= 1000:\n",
    "            break\n",
    "\n",
    "        # Increment epoch counter\n",
    "        epoch_count += 1\n",
    "\n",
    "    # Print final weights and bias\n",
    "    print(f\"Activation Type: {activation_type}\")\n",
    "    print(f\"Final Weights and Bias after {epoch_count} epochs:\")\n",
    "    print(f\"W0 (Bias): {W0}, W1: {W1}, W2: {W2}\")\n",
    "\n",
    "    # Plotting the sum-square error versus epochs\n",
    "    plt.plot(epochs, errors, label=f\"{activation_type} Activation - SSE\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Sum-Square Error\")\n",
    "    plt.title(f\"Perceptron Training - {activation_type} Activation\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return epoch_count, errors\n",
    "\n",
    "\n",
    "def perceptron_with_learning_rate_variation():\n",
    "    \"\"\"\n",
    "    A perceptron to learn AND gate logic with varying learning rates, \n",
    "    keeping the initial weights the same. The function will plot \n",
    "    the number of iterations taken for learning to converge \n",
    "    against the learning rates.\n",
    "    \"\"\"\n",
    "    # Define the learning rates to test\n",
    "    learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    \n",
    "    # Initialize weights and bias (these will remain the same for each learning rate)\n",
    "    W0_init = 10    # Bias\n",
    "    W1_init = 0.2   # Weight for first input\n",
    "    W2_init = -0.75  # Weight for second input\n",
    "\n",
    "    # Training data for AND gate (input1, input2, expected_output)\n",
    "    training_data = np.array([\n",
    "        [0, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 1]\n",
    "    ])\n",
    "\n",
    "    # Step activation function\n",
    "    def step_activation(weighted_sum):\n",
    "        return 1 if weighted_sum >= 0 else 0\n",
    "\n",
    "    # List to store the number of iterations for each learning rate\n",
    "    iterations_to_converge = []\n",
    "\n",
    "    # Loop over different learning rates\n",
    "    for alpha in learning_rates:\n",
    "        # Reset weights to the initial values for each learning rate\n",
    "        W0 = W0_init\n",
    "        W1 = W1_init\n",
    "        W2 = W2_init\n",
    "\n",
    "        epoch_count = 0  # Epoch counter\n",
    "\n",
    "        # Training loop\n",
    "        while True:\n",
    "            total_error = 0\n",
    "            for row in training_data:\n",
    "                input1 = row[0]\n",
    "                input2 = row[1]\n",
    "                expected_output = row[2]\n",
    "\n",
    "                # Weighted sum: W0*1 (bias) + W1*input1 + W2*input2\n",
    "                weighted_sum = W0 + W1 * input1 + W2 * input2\n",
    "\n",
    "                # Step activation function output\n",
    "                output = step_activation(weighted_sum)\n",
    "\n",
    "                # Error calculation (difference between expected output and actual output)\n",
    "                error = expected_output - output\n",
    "\n",
    "                # Weight updates (delta rule)\n",
    "                W0 += alpha * error * 1        # Update bias (as 1 is the input to bias)\n",
    "                W1 += alpha * error * input1   # Update weight for input1\n",
    "                W2 += alpha * error * input2   # Update weight for input2\n",
    "\n",
    "                # Sum square error calculation\n",
    "                total_error += error ** 2\n",
    "\n",
    "            # Check if the learning has converged (error < 0.002) or if max epochs (1000) are reached\n",
    "            if total_error <= 0.002 or epoch_count >= 1000:\n",
    "                break\n",
    "\n",
    "            # Increment epoch counter\n",
    "            epoch_count += 1\n",
    "\n",
    "        # Store the number of epochs it took to converge for this learning rate\n",
    "        iterations_to_converge.append(epoch_count)\n",
    "\n",
    "    # Plotting the number of iterations to converge against learning rates\n",
    "    plt.plot(learning_rates, iterations_to_converge, marker='o')\n",
    "    plt.xlabel(\"Learning Rate\")\n",
    "    plt.ylabel(\"Iterations to Converge\")\n",
    "    plt.title(\"Iterations to Converge vs Learning Rates\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return learning_rates, iterations_to_converge\n",
    "\n",
    "# Load the Excel file\n",
    "def load_data(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "# Summation Unit\n",
    "def summation(weights, inputs):\n",
    "    return np.dot(weights, inputs)\n",
    "\n",
    "# Activation Functions\n",
    "def step_activation(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def bipolar_step_activation(x):\n",
    "    return 1 if x >= 0 else -1\n",
    "\n",
    "def sigmoid_activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu_activation(x):\n",
    "    return max(0, x)\n",
    "\n",
    "# Error Comparator\n",
    "def compute_error(predicted, actual):\n",
    "    return actual - predicted\n",
    "\n",
    "# Perceptron Training (for XOR and Your Dataset)\n",
    "def train_perceptron(inputs, labels, activation_func, learning_rate=0.1, epochs=1000):\n",
    "    weights = np.random.randn(inputs.shape[1] + 1)  # +1 for bias\n",
    "    errors = []\n",
    "    \n",
    "    inputs_with_bias = np.insert(inputs, 0, 1, axis=1)  # Add bias\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            summation_output = summation(weights, inputs_with_bias[i])\n",
    "            predicted = activation_func(summation_output)\n",
    "            error = compute_error(predicted, labels[i])\n",
    "            weights += learning_rate * error * inputs_with_bias[i]\n",
    "            total_error += error ** 2\n",
    "        errors.append(total_error / len(inputs))\n",
    "        if total_error < 0.002:  # Early stopping criterion\n",
    "            break\n",
    "    \n",
    "    return weights, errors\n",
    "\n",
    "# Perceptron Training for XOR with Different Activations (Question 5)\n",
    "def train_xor_perceptron():\n",
    "    xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    xor_labels = np.array([0, 1, 1, 0])  # XOR outputs\n",
    "    \n",
    "    activations = {\n",
    "        \"Step\": step_activation,\n",
    "        \"Bipolar Step\": bipolar_step_activation,\n",
    "        \"Sigmoid\": sigmoid_activation,\n",
    "        \"ReLU\": relu_activation\n",
    "    }\n",
    "    \n",
    "    for activation_name, activation_func in activations.items():\n",
    "        print(f\"\\nTraining Perceptron with {activation_name} Activation Function for XOR:\")\n",
    "        weights, errors = train_perceptron(xor_inputs, xor_labels, activation_func)\n",
    "        plt.plot(errors, label=activation_name)\n",
    "        print(f\"Weights: {weights}\")\n",
    "    \n",
    "    plt.title('XOR Perceptron Training - Error vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Perceptron Training for Your Data (Question 6)\n",
    "def train_your_perceptron(data):\n",
    "    inputs = data[['embed_380', 'embed_381', 'embed_382', 'embed_383']].values\n",
    "    labels = data['output'].values\n",
    "    weights, errors = train_perceptron(inputs, labels, sigmoid_activation)\n",
    "    plt.plot(errors)\n",
    "    plt.title('Your Data Perceptron Training - Error vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "\n",
    "# Matrix Pseudo-Inverse Solution (Question 7)\n",
    "def matrix_pseudo_inverse(inputs, labels):\n",
    "    inputs_with_bias = np.insert(inputs, 0, 1, axis=1)  # Add bias\n",
    "    pseudo_inverse = np.linalg.pinv(inputs_with_bias)\n",
    "    weights = np.dot(pseudo_inverse, labels)\n",
    "    return weights\n",
    "\n",
    "# Backpropagation Neural Network for AND Gate (Question 8)\n",
    "def backpropagation_nn(inputs, labels, learning_rate=0.05, epochs=1000):\n",
    "    weights_input_hidden = np.random.randn(2, 2)  # Two inputs, two hidden units\n",
    "    weights_hidden_output = np.random.randn(2)\n",
    "    bias_hidden = np.random.randn(2)\n",
    "    bias_output = np.random.randn(1)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            # Forward pass\n",
    "            hidden_input = np.dot(inputs[i], weights_input_hidden) + bias_hidden\n",
    "            hidden_output = sigmoid_activation(hidden_input)\n",
    "            final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "            final_output = sigmoid_activation(final_input)\n",
    "            \n",
    "            # Error calculation\n",
    "            error = labels[i] - final_output\n",
    "            total_error += error ** 2\n",
    "            \n",
    "            # Backpropagation\n",
    "            d_output = error * final_output * (1 - final_output)\n",
    "            d_hidden = d_output * weights_hidden_output * hidden_output * (1 - hidden_output)\n",
    "            \n",
    "            # Update weights\n",
    "            weights_hidden_output += learning_rate * d_output * hidden_output\n",
    "            bias_output += learning_rate * d_output\n",
    "            weights_input_hidden += learning_rate * np.outer(inputs[i], d_hidden)\n",
    "            bias_hidden += learning_rate * d_hidden\n",
    "        \n",
    "        errors.append(total_error / len(inputs))\n",
    "        if total_error < 0.002:\n",
    "            break\n",
    "    \n",
    "    return weights_input_hidden, weights_hidden_output, errors\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# XOR gate inputs and labels\n",
    "inputs_XOR = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR gate inputs\n",
    "labels_XOR = np.array([0, 1, 1, 0])  # XOR gate outputs\n",
    "\n",
    "# Backpropagation function for XOR\n",
    "def backpropagation_nn(inputs, labels, learning_rate=0.05, epochs=1000):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    weights_input_hidden = np.random.randn(2, 2)  # Two inputs, two hidden units\n",
    "    weights_hidden_output = np.random.randn(2)\n",
    "    bias_hidden = np.random.randn(2)\n",
    "    bias_output = np.random.randn(1)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            # Forward pass\n",
    "            hidden_input = np.dot(inputs[i], weights_input_hidden) + bias_hidden\n",
    "            hidden_output = sigmoid(hidden_input)\n",
    "            final_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
    "            final_output = sigmoid(final_input)\n",
    "            \n",
    "            # Error calculation\n",
    "            error = labels[i] - final_output\n",
    "            total_error += error ** 2\n",
    "            \n",
    "            # Backpropagation\n",
    "            d_output = error * final_output * (1 - final_output)\n",
    "            d_hidden = d_output * weights_hidden_output * hidden_output * (1 - hidden_output)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            weights_hidden_output += learning_rate * d_output * hidden_output\n",
    "            bias_output += learning_rate * d_output\n",
    "            weights_input_hidden += learning_rate * np.outer(inputs[i], d_hidden)\n",
    "            bias_hidden += learning_rate * d_hidden\n",
    "        \n",
    "        errors.append(total_error / len(inputs))\n",
    "        if total_error < 0.002:\n",
    "            break\n",
    "    return weights_input_hidden, weights_hidden_output, errors\n",
    "\n",
    "def perceptron_two_outputs(inputs, targets, learning_rate=0.05, epochs=1000):\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    input_size = inputs.shape[1]  # Number of input features\n",
    "    output_size = 2  # Two output nodes\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    weights = np.random.randn(input_size, output_size)  # Random weights for inputs to outputs\n",
    "    biases = np.random.randn(output_size)  # Random biases for output nodes\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        for i in range(len(inputs)):\n",
    "            # Forward pass\n",
    "            summation = np.dot(inputs[i], weights) + biases\n",
    "            output = sigmoid(summation)  # Apply sigmoid activation function\n",
    "\n",
    "            # Calculate error (target - output)\n",
    "            error = targets[i] - output\n",
    "            total_error += np.sum(error ** 2)\n",
    "\n",
    "            # Backpropagation / Update weights and biases\n",
    "            delta = error * output * (1 - output)  # Derivative of sigmoid\n",
    "            weights += learning_rate * np.outer(inputs[i], delta)  # Update weights\n",
    "            biases += learning_rate * delta  # Update biases\n",
    "\n",
    "        errors.append(total_error / len(inputs))  # Mean squared error per epoch\n",
    "        if total_error < 0.002:\n",
    "            break\n",
    "\n",
    "    return weights, biases, errors\n",
    "\n",
    "def train_and_xor_gates(inputs_AND, targets_AND, inputs_XOR, targets_XOR):\n",
    "    # AND Gate using MLPClassifier\n",
    "    mlp_and = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', solver='adam', max_iter=1000, learning_rate_init=0.05)\n",
    "    mlp_and.fit(inputs_AND[:, 1:], targets_AND.ravel())  # Training without bias column\n",
    "    and_prediction = mlp_and.predict(inputs_AND[:, 1:])\n",
    "\n",
    "    # XOR Gate using MLPClassifier\n",
    "    mlp_xor = MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', solver='adam', max_iter=1000, learning_rate_init=0.05)\n",
    "    mlp_xor.fit(inputs_XOR[:, 1:], targets_XOR.ravel())  # Training without bias column\n",
    "    xor_prediction = mlp_xor.predict(inputs_XOR[:, 1:])\n",
    "\n",
    "    return and_prediction, xor_prediction\n",
    "\n",
    "def train_and_predict_mlp(df):\n",
    "    \"\"\"\n",
    "    Trains an MLPClassifier on the customer dataset and returns predictions.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataset containing embedding features and output target.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The predicted classifications for the input dataset.\n",
    "    \"\"\"\n",
    "    # Separate features (embeddings) and target (output)\n",
    "    customers = df[['embed_{}'.format(i) for i in range(384)]]  # Features: embed_0 to embed_383\n",
    "    \n",
    "    # Convert output to binary classes (0 or 1)\n",
    "    df['output'] = df['output'].apply(lambda x: 1 if x > 4 else 0)\n",
    "    \n",
    "    targets = df['output']  # Target: output\n",
    "\n",
    "    # Initialize the MLPClassifier\n",
    "    mlp_txn = MLPClassifier(hidden_layer_sizes=(5,), activation='logistic', solver='adam', max_iter=1000, learning_rate_init=0.05)\n",
    "\n",
    "    # Train the model\n",
    "    mlp_txn.fit(customers, targets)\n",
    "\n",
    "    # Predict and return classification results\n",
    "    predictions = mlp_txn.predict(customers)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataset_path = r\"C:\\Users\\Admin\\Downloads\\training_mathbert 4.xlsx\"\n",
    "    training_data = load_data(dataset_path)\n",
    "    function_modules(dataset_path)\n",
    "    # Call the function to train the perceptron\n",
    "    perceptron_and_gate()\n",
    "    # Running the experiment for all three activation functions\n",
    "    for activation in ['bi-polar', 'sigmoid', 'relu']:\n",
    "        perceptron_with_activation(activation)\n",
    "    # Call the function to train the perceptron with varying learning rates\n",
    "    perceptron_with_learning_rate_variation()\n",
    "    # Question 5 - XOR Gate Perceptron Training\n",
    "    print(\"Running Question 5 - XOR Gate Perceptron Training\")\n",
    "    train_xor_perceptron()\n",
    "    \n",
    "    # Question 6 - Your Data Perceptron Training\n",
    "    print(\"\\nRunning Question 6 - Your Data Perceptron Training\")\n",
    "    train_your_perceptron(training_data)\n",
    "    \n",
    "    # Question 7 - Matrix Pseudo-Inverse Comparison for XOR\n",
    "    xor_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    xor_labels = np.array([0, 1, 1, 0])  # XOR outputs\n",
    "    print(\"\\nRunning Question 7 - Matrix Pseudo-Inverse Comparison for XOR\")\n",
    "    pseudo_inverse_weights = matrix_pseudo_inverse(xor_inputs, xor_labels)\n",
    "    print(f\"Pseudo-Inverse Weights for XOR: {pseudo_inverse_weights}\")\n",
    "    \n",
    "    # Question 8 - Backpropagation Neural Network for AND Gate\n",
    "    and_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    and_labels = np.array([0, 0, 0, 1])  # AND outputs\n",
    "    print(\"\\nRunning Question 8 - Backpropagation Neural Network for AND Gate\")\n",
    "    weights_input_hidden, weights_hidden_output, errors = backpropagation_nn(and_inputs, and_labels)\n",
    "    print(f\"Backpropagation Weights (Input-Hidden): {weights_input_hidden}\")\n",
    "    print(f\"Backpropagation Weights (Hidden-Output): {weights_hidden_output}\")\n",
    "    plt.plot(errors)\n",
    "    plt.title('AND Gate Backpropagation Neural Network - Error vs Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.show()\n",
    "\n",
    "    # Define inputs_XOR and labels_XOR\n",
    "    inputs_XOR = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    labels_XOR = np.array([0, 1, 1, 0])\n",
    "\n",
    "    weights_input_hidden, weights_hidden_output, errors = backpropagation_nn(inputs_XOR, labels_XOR)\n",
    "\n",
    "    # Plot the error over epochs\n",
    "    plt.plot(errors)\n",
    "    plt.title(\"Epochs vs Error for XOR Gate\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "    # Final weights\n",
    "    print(\"Final weights (input to hidden):\", weights_input_hidden)\n",
    "    print(\"Final weights (hidden to output):\", weights_hidden_output)\n",
    "\n",
    "    # AND gate input and output (two output nodes)\n",
    "    inputs_AND = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # AND gate inputs\n",
    "    targets_AND = np.array([[1, 0], [1, 0], [1, 0], [0, 1]])  # AND gate targets (2 output nodes)\n",
    "\n",
    "    # Train perceptron on AND gate\n",
    "    weights_AND, biases_AND, errors_AND = perceptron_two_outputs(inputs_AND, targets_AND)\n",
    "\n",
    "    # Plot the error for AND gate\n",
    "    plt.plot(errors_AND)\n",
    "    plt.title(\"Epochs vs Error for AND Gate (Two Output Nodes)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "    # XOR gate input and output (two output nodes)\n",
    "    inputs_XOR = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # XOR gate inputs\n",
    "    targets_XOR = np.array([[1, 0], [0, 1], [0, 1], [1, 0]])  # XOR gate targets (2 output nodes)\n",
    "\n",
    "    # Train perceptron on XOR gate\n",
    "    weights_XOR, biases_XOR, errors_XOR = perceptron_two_outputs(inputs_XOR, targets_XOR)\n",
    "\n",
    "    # Plot the error for XOR gate\n",
    "    plt.plot(errors_XOR)\n",
    "    plt.title(\"Epochs vs Error for XOR Gate (Two Output Nodes)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    plt.show()\n",
    "\n",
    "    # Example usage:\n",
    "    inputs_AND_nn = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    targets_AND_nn = np.array([0, 0, 0, 1])\n",
    "\n",
    "    inputs_XOR_nn = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    targets_XOR_nn = np.array([0, 1, 1, 0])\n",
    "\n",
    "    and_prediction, xor_prediction = train_and_xor_gates(inputs_AND_nn, targets_AND_nn, inputs_XOR_nn, targets_XOR_nn)\n",
    "    print(\"AND Gate Prediction (MLP):\", and_prediction)\n",
    "    print(\"XOR Gate Prediction (MLP):\", xor_prediction)\n",
    "\n",
    "    df = pd.read_excel(dataset_path)\n",
    "# Predict using the MLP model\n",
    "    predictions = train_and_predict_mlp(df)\n",
    "# Output predictions\n",
    "    print(\"Transaction Classification (MLP):\", predictions)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
