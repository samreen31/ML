{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86122a3-0c91-4834-8913-b45e43aa2fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\myenv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:58:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance Comparison:\n",
      "\n",
      "           Model  Accuracy  Precision    Recall  F1-Score   AUC-ROC\n",
      "0            MLP  0.805310   0.509804  0.577778  0.541667  0.719828\n",
      "1            SVM  0.800885   0.500000  0.400000  0.444444  0.650276\n",
      "2  Decision Tree  0.734513   0.363636  0.444444  0.400000  0.625537\n",
      "3  Random Forest  0.836283   0.666667  0.355556  0.463768  0.655678\n",
      "4       AdaBoost  0.787611   0.461538  0.400000  0.428571  0.641989\n",
      "5        XGBoost  0.836283   0.611111  0.488889  0.543210  0.705770\n",
      "6       CatBoost  0.853982   0.730769  0.422222  0.535211  0.691774\n",
      "7    Naïve Bayes  0.663717   0.308642  0.555556  0.396825  0.623082\n",
      "\n",
      "MLPClassifier with RandomizedSearchCV Tuning:\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Function to calculate and return performance metrics\n",
    "def evaluate_model(y_test, y_pred, model_name):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred)\n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'AUC-ROC': auc_roc\n",
    "    }\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "def train_and_evaluate_classifiers(df):\n",
    "    # Separate features (embeddings) and target (output)\n",
    "    customers = df[['embed_{}'.format(i) for i in range(384)]]\n",
    "    \n",
    "    # Convert output to binary classes (0 or 1)\n",
    "    df['output'] = df['output'].apply(lambda x: 1 if x > 4 else 0)\n",
    "    targets = df['output']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(customers, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize models\n",
    "    models = {\n",
    "        \"MLP\": MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1000, solver='adam', learning_rate_init=0.01),\n",
    "        \"SVM\": SVC(kernel='linear', probability=True),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(),\n",
    "        \"AdaBoost\": AdaBoostClassifier(),\n",
    "        \"XGBoost\": XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "        \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "        \"Naïve Bayes\": GaussianNB()\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    # Train each model and evaluate\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        result = evaluate_model(y_test, y_pred, model_name)\n",
    "        results.append(result)\n",
    "    \n",
    "    # Create DataFrame to tabulate results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "# MLPClassifier with RandomizedSearchCV tuning\n",
    "def train_and_predict_mlp(df):\n",
    "    \"\"\"\n",
    "    Trains an MLPClassifier using RandomizedSearchCV for hyperparameter tuning, and returns predictions.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    customers = df[['embed_{}'.format(i) for i in range(384)]]\n",
    "    df['output'] = df['output'].apply(lambda x: 1 if x > 4 else 0)\n",
    "    targets = df['output']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(customers, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define MLPClassifier\n",
    "    mlp = MLPClassifier()\n",
    "\n",
    "    # Define hyperparameter grid for RandomizedSearchCV\n",
    "    param_grid_mlp = {\n",
    "        'hidden_layer_sizes': [(5,), (10,), (50, 50), (100, 100)],\n",
    "        'activation': ['logistic', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [1e-4, 1e-3, 1e-2],\n",
    "        'learning_rate_init': [0.001, 0.01, 0.05],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    }\n",
    "\n",
    "    # Initialize RandomizedSearchCV for MLP\n",
    "    random_search_mlp = RandomizedSearchCV(mlp, param_grid_mlp, n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Fit RandomizedSearchCV with training data\n",
    "    random_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_mlp = random_search_mlp.best_estimator_\n",
    "\n",
    "    # Predict on the test set\n",
    "    predictions = best_mlp.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Best Parameters found: \", random_search_mlp.best_params_)\n",
    "    print(\"Accuracy on test set: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "    # Return predictions\n",
    "    return predictions\n",
    "\n",
    "def main():\n",
    "    dataset_path = r\"C:\\Users\\Admin\\Downloads\\training_mathbert 4.xlsx\"\n",
    "    df = pd.read_excel(dataset_path)\n",
    "\n",
    "    # Train and evaluate all classifiers\n",
    "    results_df = train_and_evaluate_classifiers(df)\n",
    "\n",
    "    # Print comparison table\n",
    "    print(\"Model Performance Comparison:\\n\")\n",
    "    print(results_df)\n",
    "\n",
    "    # Train MLP with hyperparameter tuning and print results\n",
    "    print(\"\\nMLPClassifier with RandomizedSearchCV Tuning:\")\n",
    "    predictions = train_and_predict_mlp(df)\n",
    "    print(\"Transaction Classification (MLP):\", predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416e755-ee2c-4704-b3ec-d3b518937ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
